{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BertModel in module transformers.models.bert.modeling_bert object:\n",
      "\n",
      "class BertModel(BertPreTrainedModel)\n",
      " |  BertModel(config, add_pooling_layer=True)\n",
      " |  \n",
      " |  The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\n",
      " |  \n",
      " |  This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
      " |  methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
      " |  pruning heads etc.)\n",
      " |  \n",
      " |  This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
      " |  subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
      " |  general usage and behavior.\n",
      " |  \n",
      " |  Parameters:\n",
      " |      config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n",
      " |          Initializing with a config file does not load the weights associated with the model, only the\n",
      " |          configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
      " |          weights.\n",
      " |  \n",
      " |  \n",
      " |  The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
      " |  cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
      " |  all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
      " |  Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
      " |  \n",
      " |  To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
      " |  set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
      " |  argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
      " |  input to the forward pass.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BertModel\n",
      " |      BertPreTrainedModel\n",
      " |      transformers.modeling_utils.PreTrainedModel\n",
      " |      torch.nn.modules.module.Module\n",
      " |      transformers.modeling_utils.ModuleUtilsMixin\n",
      " |      transformers.generation_utils.GenerationMixin\n",
      " |      transformers.file_utils.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, config, add_pooling_layer=True)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)\n",
      " |      The :class:`~transformers.BertModel` forward method, overrides the :func:`__call__` special method.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within this function, one should call the\n",
      " |          :class:`Module` instance afterwards instead of this since the former takes care of running the pre and post\n",
      " |          processing steps while the latter silently ignores them.\n",
      " |          \n",
      " |      Args:\n",
      " |          input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
      " |              Indices of input sequence tokens in the vocabulary.\n",
      " |      \n",
      " |              Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n",
      " |              :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
      " |              details.\n",
      " |      \n",
      " |              `What are input IDs? <../glossary.html#input-ids>`__\n",
      " |          attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      " |              Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
      " |      \n",
      " |              - 1 for tokens that are **not masked**,\n",
      " |              - 0 for tokens that are **masked**.\n",
      " |      \n",
      " |              `What are attention masks? <../glossary.html#attention-mask>`__\n",
      " |          token_type_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      " |              Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
      " |              1]``:\n",
      " |      \n",
      " |              - 0 corresponds to a `sentence A` token,\n",
      " |              - 1 corresponds to a `sentence B` token.\n",
      " |      \n",
      " |              `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
      " |          position_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      " |              Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
      " |              config.max_position_embeddings - 1]``.\n",
      " |      \n",
      " |              `What are position IDs? <../glossary.html#position-ids>`_\n",
      " |          head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
      " |              Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
      " |      \n",
      " |              - 1 indicates the head is **not masked**,\n",
      " |              - 0 indicates the head is **masked**.\n",
      " |      \n",
      " |          inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
      " |              Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
      " |              This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
      " |              vectors than the model's internal embedding lookup matrix.\n",
      " |          output_attentions (:obj:`bool`, `optional`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
      " |              tensors for more detail.\n",
      " |          output_hidden_states (:obj:`bool`, `optional`):\n",
      " |              Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
      " |              more detail.\n",
      " |          return_dict (:obj:`bool`, `optional`):\n",
      " |              Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      " |      \n",
      " |          encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
      " |              Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
      " |              the model is configured as a decoder.\n",
      " |          encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      " |              Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
      " |              the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
      " |      \n",
      " |              - 1 for tokens that are **not masked**,\n",
      " |              - 0 for tokens that are **masked**.\n",
      " |          past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
      " |              Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
      " |      \n",
      " |              If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
      " |              (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
      " |              instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
      " |          use_cache (:obj:`bool`, `optional`):\n",
      " |              If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
      " |              decoding (see :obj:`past_key_values`).\n",
      " |          \n",
      " |      Returns:\n",
      " |          :class:`~transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions` or :obj:`tuple(torch.FloatTensor)`: A :class:`~transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions` or a tuple of\n",
      " |          :obj:`torch.FloatTensor` (if ``return_dict=False`` is passed or when ``config.return_dict=False``) comprising\n",
      " |          various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs.\n",
      " |      \n",
      " |          - **last_hidden_state** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the model.\n",
      " |          - **pooler_output** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`) -- Last layer hidden-state of the first token of the sequence (classification token) after further processing\n",
      " |            through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns\n",
      " |            the classification token after processing through a linear layer and a tanh activation function. The linear\n",
      " |            layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n",
      " |          - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``) -- Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
      " |            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
      " |      \n",
      " |            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
      " |          - **attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
      " |            sequence_length, sequence_length)`.\n",
      " |      \n",
      " |            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      " |            heads.\n",
      " |          - **cross_attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
      " |            sequence_length, sequence_length)`.\n",
      " |      \n",
      " |            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
      " |            weighted average in the cross-attention heads.\n",
      " |          - **past_key_values** (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``) -- Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
      " |            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      " |            ``config.is_encoder_decoder=True`` 2 additional tensors of shape :obj:`(batch_size, num_heads,\n",
      " |            encoder_sequence_length, embed_size_per_head)`.\n",
      " |      \n",
      " |            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
      " |            ``config.is_encoder_decoder=True`` in the cross-attention blocks) that can be used (see\n",
      " |            :obj:`past_key_values` input) to speed up sequential decoding.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> from transformers import BertTokenizer, BertModel\n",
      " |          >>> import torch\n",
      " |      \n",
      " |          >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      " |          >>> model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |      \n",
      " |          >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
      " |          >>> outputs = model(**inputs)\n",
      " |      \n",
      " |          >>> last_hidden_states = outputs.last_hidden_state\n",
      " |  \n",
      " |  get_input_embeddings(self)\n",
      " |      Returns the model's input embeddings.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`nn.Module`: A torch module mapping vocabulary to hidden states.\n",
      " |  \n",
      " |  set_input_embeddings(self, value)\n",
      " |      Set model's input embeddings.\n",
      " |      \n",
      " |      Args:\n",
      " |          value (:obj:`nn.Module`): A module mapping vocabulary to hidden states.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BertPreTrainedModel:\n",
      " |  \n",
      " |  load_tf_weights = load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n",
      " |      Load tf checkpoints in a pytorch model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BertPreTrainedModel:\n",
      " |  \n",
      " |  base_model_prefix = 'bert'\n",
      " |  \n",
      " |  config_class = <class 'transformers.models.bert.configuration_bert.Ber...\n",
      " |      This is the configuration class to store the configuration of a :class:`~transformers.BertModel` or a\n",
      " |      :class:`~transformers.TFBertModel`. It is used to instantiate a BERT model according to the specified arguments,\n",
      " |      defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration\n",
      " |      to that of the BERT `bert-base-uncased <https://huggingface.co/bert-base-uncased>`__ architecture.\n",
      " |      \n",
      " |      Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model\n",
      " |      outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          vocab_size (:obj:`int`, `optional`, defaults to 30522):\n",
      " |              Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the\n",
      " |              :obj:`inputs_ids` passed when calling :class:`~transformers.BertModel` or\n",
      " |              :class:`~transformers.TFBertModel`.\n",
      " |          hidden_size (:obj:`int`, `optional`, defaults to 768):\n",
      " |              Dimensionality of the encoder layers and the pooler layer.\n",
      " |          num_hidden_layers (:obj:`int`, `optional`, defaults to 12):\n",
      " |              Number of hidden layers in the Transformer encoder.\n",
      " |          num_attention_heads (:obj:`int`, `optional`, defaults to 12):\n",
      " |              Number of attention heads for each attention layer in the Transformer encoder.\n",
      " |          intermediate_size (:obj:`int`, `optional`, defaults to 3072):\n",
      " |              Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n",
      " |          hidden_act (:obj:`str` or :obj:`Callable`, `optional`, defaults to :obj:`\"gelu\"`):\n",
      " |              The non-linear activation function (function or string) in the encoder and pooler. If string,\n",
      " |              :obj:`\"gelu\"`, :obj:`\"relu\"`, :obj:`\"silu\"` and :obj:`\"gelu_new\"` are supported.\n",
      " |          hidden_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
      " |              The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
      " |          attention_probs_dropout_prob (:obj:`float`, `optional`, defaults to 0.1):\n",
      " |              The dropout ratio for the attention probabilities.\n",
      " |          max_position_embeddings (:obj:`int`, `optional`, defaults to 512):\n",
      " |              The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
      " |              just in case (e.g., 512 or 1024 or 2048).\n",
      " |          type_vocab_size (:obj:`int`, `optional`, defaults to 2):\n",
      " |              The vocabulary size of the :obj:`token_type_ids` passed when calling :class:`~transformers.BertModel` or\n",
      " |              :class:`~transformers.TFBertModel`.\n",
      " |          initializer_range (:obj:`float`, `optional`, defaults to 0.02):\n",
      " |              The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
      " |          layer_norm_eps (:obj:`float`, `optional`, defaults to 1e-12):\n",
      " |              The epsilon used by the layer normalization layers.\n",
      " |          position_embedding_type (:obj:`str`, `optional`, defaults to :obj:`\"absolute\"`):\n",
      " |              Type of position embedding. Choose one of :obj:`\"absolute\"`, :obj:`\"relative_key\"`,\n",
      " |              :obj:`\"relative_key_query\"`. For positional embeddings use :obj:`\"absolute\"`. For more information on\n",
      " |              :obj:`\"relative_key\"`, please refer to `Self-Attention with Relative Position Representations (Shaw et al.)\n",
      " |              <https://arxiv.org/abs/1803.02155>`__. For more information on :obj:`\"relative_key_query\"`, please refer to\n",
      " |              `Method 4` in `Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)\n",
      " |              <https://arxiv.org/abs/2009.13658>`__.\n",
      " |          use_cache (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
      " |              relevant if ``config.is_decoder=True``.\n",
      " |          classifier_dropout (:obj:`float`, `optional`):\n",
      " |              The dropout ratio for the classification head.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> from transformers import BertModel, BertConfig\n",
      " |      \n",
      " |          >>> # Initializing a BERT bert-base-uncased style configuration\n",
      " |          >>> configuration = BertConfig()\n",
      " |      \n",
      " |          >>> # Initializing a model from the bert-base-uncased style configuration\n",
      " |          >>> model = BertModel(configuration)\n",
      " |      \n",
      " |          >>> # Accessing the model configuration\n",
      " |          >>> configuration = model.config\n",
      " |  \n",
      " |  \n",
      " |  supports_gradient_checkpointing = True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  get_output_embeddings(self) -> torch.nn.modules.module.Module\n",
      " |      Returns the model's output embeddings.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`nn.Module`: A torch module mapping hidden states to vocabulary.\n",
      " |  \n",
      " |  get_position_embeddings(self) -> Union[torch.nn.modules.sparse.Embedding, Tuple[torch.nn.modules.sparse.Embedding]]\n",
      " |  \n",
      " |  gradient_checkpointing_disable(self)\n",
      " |      Deactivates gradient checkpointing for the current model.\n",
      " |      \n",
      " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
      " |      activations\".\n",
      " |  \n",
      " |  gradient_checkpointing_enable(self)\n",
      " |      Activates gradient checkpointing for the current model.\n",
      " |      \n",
      " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
      " |      activations\".\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      If needed prunes and maybe initializes weights.\n",
      " |  \n",
      " |  post_init(self)\n",
      " |      A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n",
      " |      modules properly initialized (such as weight initialization).\n",
      " |  \n",
      " |  prune_heads(self, heads_to_prune: Dict[int, List[int]])\n",
      " |      Prunes heads of the base model.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          heads_to_prune (:obj:`Dict[int, List[int]]`):\n",
      " |              Dictionary with keys being selected layer indices (:obj:`int`) and associated values being the list of\n",
      " |              heads to prune in said layer (list of :obj:`int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads\n",
      " |              0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n",
      " |  \n",
      " |  push_to_hub(self, repo_path_or_name: Optional[str] = None, repo_url: Optional[str] = None, use_temp_dir: bool = False, commit_message: Optional[str] = None, organization: Optional[str] = None, private: Optional[bool] = None, use_auth_token: Union[bool, str, NoneType] = None, **model_card_kwargs) -> str\n",
      " |      Upload the model checkpoint to the 🤗 Model Hub while synchronizing a local clone of the repo in\n",
      " |      :obj:`repo_path_or_name`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_path_or_name (:obj:`str`, `optional`):\n",
      " |              Can either be a repository name for your model in the Hub or a path to a local folder (in which case\n",
      " |              the repository will have the name of that local folder). If not specified, will default to the name\n",
      " |              given by :obj:`repo_url` and a local directory with that name will be created.\n",
      " |          repo_url (:obj:`str`, `optional`):\n",
      " |              Specify this in case you want to push to an existing repository in the hub. If unspecified, a new\n",
      " |              repository will be created in your namespace (unless you specify an :obj:`organization`) with\n",
      " |              :obj:`repo_name`.\n",
      " |          use_temp_dir (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to clone the distant repo in a temporary directory or in :obj:`repo_path_or_name` inside\n",
      " |              the current working directory. This will slow things down if you are making changes in an existing repo\n",
      " |              since you will need to clone the repo before every push.\n",
      " |          commit_message (:obj:`str`, `optional`):\n",
      " |              Message to commit while pushing. Will default to :obj:`\"add model\"`.\n",
      " |          organization (:obj:`str`, `optional`):\n",
      " |              Organization in which you want to push your model (you must be a member of this organization).\n",
      " |          private (:obj:`bool`, `optional`):\n",
      " |              Whether or not the repository created should be private (requires a paying subscription).\n",
      " |          use_auth_token (:obj:`bool` or :obj:`str`, `optional`):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      " |              generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`). Will default to\n",
      " |              :obj:`True` if :obj:`repo_url` is not specified.\n",
      " |      \n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`str`: The url of the commit of your model in the given repository.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          from transformers import AutoModel\n",
      " |      \n",
      " |          model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
      " |      \n",
      " |          # Push the model to your namespace with the name \"my-finetuned-bert\" and have a local clone in the\n",
      " |          # `my-finetuned-bert` folder.\n",
      " |          model.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |          # Push the model to your namespace with the name \"my-finetuned-bert\" with no local clone.\n",
      " |          model.push_to_hub(\"my-finetuned-bert\", use_temp_dir=True)\n",
      " |      \n",
      " |          # Push the model to an organization with the name \"my-finetuned-bert\" and have a local clone in the\n",
      " |          # `my-finetuned-bert` folder.\n",
      " |          model.push_to_hub(\"my-finetuned-bert\", organization=\"huggingface\")\n",
      " |      \n",
      " |          # Make a change to an existing repo that has been cloned locally in `my-finetuned-bert`.\n",
      " |          model.push_to_hub(\"my-finetuned-bert\", repo_url=\"https://huggingface.co/sgugger/my-finetuned-bert\")\n",
      " |  \n",
      " |  resize_position_embeddings(self, new_num_position_embeddings: int)\n",
      " |  \n",
      " |  resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> torch.nn.modules.sparse.Embedding\n",
      " |      Resizes input token embeddings matrix of the model if :obj:`new_num_tokens != config.vocab_size`.\n",
      " |      \n",
      " |      Takes care of tying weights embeddings afterwards if the model class has a :obj:`tie_weights()` method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          new_num_tokens (:obj:`int`, `optional`):\n",
      " |              The number of new tokens in the embedding matrix. Increasing the size will add newly initialized\n",
      " |              vectors at the end. Reducing the size will remove vectors from the end. If not provided or :obj:`None`,\n",
      " |              just returns a pointer to the input tokens :obj:`torch.nn.Embedding` module of the model without doing\n",
      " |              anything.\n",
      " |      \n",
      " |      Return:\n",
      " |          :obj:`torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n",
      " |  \n",
      " |  retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], save_config: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x12efe94c0>, push_to_hub: bool = False, **kwargs)\n",
      " |      Save a model and its configuration file to a directory, so that it can be re-loaded using the\n",
      " |      `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (:obj:`str` or :obj:`os.PathLike`):\n",
      " |              Directory to which to save. Will be created if it doesn't exist.\n",
      " |          save_config (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to save the config of the model. Useful when in distributed training like TPUs and need\n",
      " |              to call this function on all processes. In this case, set :obj:`save_config=True` only on the main\n",
      " |              process to avoid race conditions.\n",
      " |          state_dict (nested dictionary of :obj:`torch.Tensor`):\n",
      " |              The state dictionary of the model to save. Will default to :obj:`self.state_dict()`, but can be used to\n",
      " |              only save parts of the model or if special precautions need to be taken when recovering the state\n",
      " |              dictionary of a model (like when using model parallelism).\n",
      " |          save_function (:obj:`Callable`):\n",
      " |              The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n",
      " |              need to replace :obj:`torch.save` by another method.\n",
      " |          push_to_hub (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it.\n",
      " |      \n",
      " |              .. warning::\n",
      " |      \n",
      " |                  Using :obj:`push_to_hub=True` will synchronize the repository you are pushing to with\n",
      " |                  :obj:`save_directory`, which requires :obj:`save_directory` to be a local clone of the repo you are\n",
      " |                  pushing to if it's an existing folder. Pass along :obj:`temp_dir=True` to use a temporary directory\n",
      " |                  instead.\n",
      " |      \n",
      " |          kwargs:\n",
      " |              Additional key word arguments passed along to the\n",
      " |              :meth:`~transformers.file_utils.PushToHubMixin.push_to_hub` method.\n",
      " |  \n",
      " |  tie_weights(self)\n",
      " |      Tie the weights between the input embeddings and the output embeddings.\n",
      " |      \n",
      " |      If the :obj:`torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning\n",
      " |      the weights instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, **kwargs) from builtins.type\n",
      " |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      " |      \n",
      " |      The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated). To\n",
      " |      train the model, you should first set it back in training mode with ``model.train()``.\n",
      " |      \n",
      " |      The warning `Weights from XXX not initialized from pretrained model` means that the weights of XXX do not come\n",
      " |      pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n",
      " |      task.\n",
      " |      \n",
      " |      The warning `Weights from XXX not used in YYY` means that the layer XXX is not used by YYY, therefore those\n",
      " |      weights are discarded.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (:obj:`str` or :obj:`os.PathLike`, `optional`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n",
      " |                    Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n",
      " |                    a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
      " |                  - A path to a `directory` containing model weights saved using\n",
      " |                    :func:`~transformers.PreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n",
      " |                  - A path or url to a `tensorflow index checkpoint file` (e.g, ``./tf_model/model.ckpt.index``). In\n",
      " |                    this case, ``from_tf`` should be set to :obj:`True` and a configuration object should be provided\n",
      " |                    as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in\n",
      " |                    a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
      " |                  - A path or url to a model folder containing a `flax checkpoint file` in `.msgpack` format (e.g,\n",
      " |                    ``./flax_model/`` containing ``flax_model.msgpack``). In this case, ``from_flax`` should be set\n",
      " |                    to :obj:`True`.\n",
      " |                  - :obj:`None` if you are both providing the configuration and state dictionary (resp. with keyword\n",
      " |                    arguments ``config`` and ``state_dict``).\n",
      " |          model_args (sequence of positional arguments, `optional`):\n",
      " |              All remaining positional arguments will be passed to the underlying model's ``__init__`` method.\n",
      " |          config (:obj:`Union[PretrainedConfig, str, os.PathLike]`, `optional`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - an instance of a class derived from :class:`~transformers.PretrainedConfig`,\n",
      " |                  - a string or path valid as input to :func:`~transformers.PretrainedConfig.from_pretrained`.\n",
      " |      \n",
      " |              Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
      " |              be automatically loaded when:\n",
      " |      \n",
      " |                  - The model is a model provided by the library (loaded with the `model id` string of a pretrained\n",
      " |                    model).\n",
      " |                  - The model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded\n",
      " |                    by supplying the save directory.\n",
      " |                  - The model is loaded by supplying a local directory as ``pretrained_model_name_or_path`` and a\n",
      " |                    configuration JSON file named `config.json` is found in the directory.\n",
      " |          state_dict (:obj:`Dict[str, torch.Tensor]`, `optional`):\n",
      " |              A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
      " |      \n",
      " |              This option can be used if you want to create a model from a pretrained configuration but load your own\n",
      " |              weights. In this case though, you should check if using\n",
      " |              :func:`~transformers.PreTrainedModel.save_pretrained` and\n",
      " |              :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n",
      " |          cache_dir (:obj:`Union[str, os.PathLike]`, `optional`):\n",
      " |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          from_tf (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
      " |              ``pretrained_model_name_or_path`` argument).\n",
      " |          from_flax (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Load the model weights from a Flax checkpoint save file (see docstring of\n",
      " |              ``pretrained_model_name_or_path`` argument).\n",
      " |          ignore_mismatched_sizes (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n",
      " |              as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n",
      " |              checkpoint with 3 labels).\n",
      " |          force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      " |              file exists.\n",
      " |          proxies (:obj:`Dict[str, str]`, `optional`):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., :obj:`{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only(:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to only look at local files (i.e., do not try to download the model).\n",
      " |          use_auth_token (:obj:`str` or `bool`, `optional`):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      " |              generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
      " |          revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so ``revision`` can be any\n",
      " |              identifier allowed by git.\n",
      " |          mirror(:obj:`str`, `optional`):\n",
      " |              Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n",
      " |              problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n",
      " |              Please refer to the mirror site for more information.\n",
      " |          _fast_init(:obj:`bool`, `optional`, defaults to `:obj:`True`):\n",
      " |              Whether or not to disable fast initialization.\n",
      " |          low_cpu_mem_usage(:obj:`bool`, `optional`, defaults to `:obj:`False`):\n",
      " |              Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      " |              This is an experimental feature and a subject to change at any moment.\n",
      " |          torch_dtype (:obj:`str` or :obj:`torch.dtype`, `optional`):\n",
      " |              Override the default ``torch.dtype`` and load the model under this dtype. If ``\"auto\"`` is passed the\n",
      " |              dtype will be automatically derived from the model's weights.\n",
      " |      \n",
      " |              .. warning::\n",
      " |      \n",
      " |                  One should only disable `_fast_init` to ensure backwards compatibility with\n",
      " |                  ``transformers.__version__ < 4.6.0`` for seeded model initialization. This argument will be removed\n",
      " |                  at the next major version. See `pull request 11471\n",
      " |                  <https://github.com/huggingface/transformers/pull/11471>`__ for more information.\n",
      " |      \n",
      " |          kwargs (remaining dictionary of keyword arguments, `optional`):\n",
      " |              Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
      " |              :obj:`output_attentions=True`). Behaves differently depending on whether a ``config`` is provided or\n",
      " |              automatically loaded:\n",
      " |      \n",
      " |                  - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the\n",
      " |                    underlying model's ``__init__`` method (we assume all relevant updates to the configuration have\n",
      " |                    already been done)\n",
      " |                  - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class\n",
      " |                    initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of\n",
      " |                    ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute\n",
      " |                    with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration\n",
      " |                    attribute will be passed to the underlying model's ``__init__`` function.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          Passing :obj:`use_auth_token=True` is required when you want to use a private model.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          Activate the special `\"offline-mode\"\n",
      " |          <https://huggingface.co/transformers/installation.html#offline-mode>`__ to use this method in a firewalled\n",
      " |          environment.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> from transformers import BertConfig, BertModel\n",
      " |          >>> # Download model and configuration from huggingface.co and cache.\n",
      " |          >>> model = BertModel.from_pretrained('bert-base-uncased')\n",
      " |          >>> # Model was saved using `save_pretrained('./test/saved_model/')` (for example purposes, not runnable).\n",
      " |          >>> model = BertModel.from_pretrained('./test/saved_model/')\n",
      " |          >>> # Update configuration during loading.\n",
      " |          >>> model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
      " |          >>> assert model.config.output_attentions == True\n",
      " |          >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n",
      " |          >>> config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n",
      " |          >>> model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n",
      " |          >>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n",
      " |          >>> model = BertModel.from_pretrained('bert-base-uncased', from_flax=True)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  base_model\n",
      " |      :obj:`torch.nn.Module`: The main body of the model.\n",
      " |  \n",
      " |  dummy_inputs\n",
      " |      :obj:`Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n",
      " |  \n",
      " |  framework\n",
      " |      :str: Identifies that this is a PyTorch model.\n",
      " |  \n",
      " |  is_gradient_checkpointing\n",
      " |      Whether gradient checkpointing is activated for this model or not.\n",
      " |      \n",
      " |      Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n",
      " |      activations\".\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.modeling_utils.PreTrainedModel:\n",
      " |  \n",
      " |  is_parallelizable = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |          or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      " |  \n",
      " |  dump_patches = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
      " |  \n",
      " |  add_memory_hooks(self)\n",
      " |      Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n",
      " |      \n",
      " |      Increase in memory consumption is stored in a :obj:`mem_rss_diff` attribute for each module and can be reset to\n",
      " |      zero with :obj:`model.reset_memory_hooks_state()`.\n",
      " |  \n",
      " |  estimate_tokens(self, input_dict: Dict[str, Union[torch.Tensor, Any]]) -> int\n",
      " |      Helper function to estimate the total number of tokens from the model inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (:obj:`dict`): The model inputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: The total number of tokens.\n",
      " |  \n",
      " |  floating_point_ops(self, input_dict: Dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True) -> int\n",
      " |      Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n",
      " |      batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n",
      " |      tokens (valid if :obj:`12 * d_model << sequence_length`) as laid out in `this paper\n",
      " |      <https://arxiv.org/pdf/2001.08361.pdf>`__ section 2.1. Should be overridden for transformers with parameter\n",
      " |      re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_size (:obj:`int`):\n",
      " |              The batch size for the forward pass.\n",
      " |      \n",
      " |          sequence_length (:obj:`int`):\n",
      " |              The number of tokens in each line of the batch.\n",
      " |      \n",
      " |          exclude_embeddings (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not to count embedding and softmax operations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: The number of floating-point operations.\n",
      " |  \n",
      " |  get_extended_attention_mask(self, attention_mask: torch.Tensor, input_shape: Tuple[int], device: <property object at 0x1322f2db0>) -> torch.Tensor\n",
      " |      Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          attention_mask (:obj:`torch.Tensor`):\n",
      " |              Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
      " |          input_shape (:obj:`Tuple[int]`):\n",
      " |              The shape of the input to the model.\n",
      " |          device: (:obj:`torch.device`):\n",
      " |              The device of the input to the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
      " |  \n",
      " |  get_head_mask(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor\n",
      " |      Prepare the head mask if needed.\n",
      " |      \n",
      " |      Args:\n",
      " |          head_mask (:obj:`torch.Tensor` with shape :obj:`[num_heads]` or :obj:`[num_hidden_layers x num_heads]`, `optional`):\n",
      " |              The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard).\n",
      " |          num_hidden_layers (:obj:`int`):\n",
      " |              The number of hidden layers in the model.\n",
      " |          is_attention_chunked: (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not the attentions scores are computed by chunks or not.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`torch.Tensor` with shape :obj:`[num_hidden_layers x batch x num_heads x seq_length x seq_length]` or\n",
      " |          list with :obj:`[None]` for each layer.\n",
      " |  \n",
      " |  invert_attention_mask(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor\n",
      " |      Invert an attention mask (e.g., switches 0. and 1.).\n",
      " |      \n",
      " |      Args:\n",
      " |          encoder_attention_mask (:obj:`torch.Tensor`): An attention mask.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`torch.Tensor`: The inverted attention mask.\n",
      " |  \n",
      " |  num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int\n",
      " |      Get number of (optionally, trainable or non-embeddings) parameters in the module.\n",
      " |      \n",
      " |      Args:\n",
      " |          only_trainable (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return only the number of trainable parameters\n",
      " |      \n",
      " |          exclude_embeddings (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to return only the number of non-embeddings parameters\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: The number of parameters.\n",
      " |  \n",
      " |  reset_memory_hooks_state(self)\n",
      " |      Reset the :obj:`mem_rss_diff` attribute of each module (see\n",
      " |      :func:`~transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.modeling_utils.ModuleUtilsMixin:\n",
      " |  \n",
      " |  device\n",
      " |      :obj:`torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n",
      " |      device).\n",
      " |  \n",
      " |  dtype\n",
      " |      :obj:`torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.generation_utils.GenerationMixin:\n",
      " |  \n",
      " |  adjust_logits_during_generation(self, logits: torch.FloatTensor, **kwargs) -> torch.FloatTensor\n",
      " |      Implement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to adjust the logits in\n",
      " |      the generate method.\n",
      " |  \n",
      " |  beam_sample(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation_beam_search.BeamScorer, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, logits_warper: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = None, **model_kwargs) -> Union[transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences for models with a language modeling head using beam search with multinomial sampling.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          beam_scorer (:obj:`BeamScorer`):\n",
      " |              A derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
      " |              constructed, stored and sorted during generation. For more information, the documentation of\n",
      " |              :class:`~transformers.BeamScorer` should be read.\n",
      " |          logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
      " |              An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
      " |              :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
      " |              head applied at each generation step.\n",
      " |          stopping_criteria (:obj:`StoppingCriteriaList`, `optional`):\n",
      " |              An instance of :class:`~transformers.StoppingCriteriaList`. List of instances of class derived from\n",
      " |              :class:`~transformers.StoppingCriteria` used to tell if the generation loop should stop.\n",
      " |          logits_warper (:obj:`LogitsProcessorList`, `optional`):\n",
      " |              An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
      " |              :class:`~transformers.LogitsWarper` used to warp the prediction score distribution of the language\n",
      " |              modeling head applied before multinomial sampling at each generation step.\n",
      " |          max_length (:obj:`int`, `optional`, defaults to 20):\n",
      " |              **DEPRECATED**. Use :obj:`logits_processor` or :obj:`stopping_criteria` directly to cap the number of\n",
      " |              generated tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `padding` token.\n",
      " |          eos_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `end-of-sequence` token.\n",
      " |          output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
      " |          return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      " |          synced_gpus (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
      " |              model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`,\n",
      " |          :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
      " |          :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput` if\n",
      " |          ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
      " |          :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput` if\n",
      " |          ``model.config.is_encoder_decoder=True``.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> from transformers import (\n",
      " |          ...     AutoTokenizer,\n",
      " |          ...     AutoModelForSeq2SeqLM,\n",
      " |          ...     LogitsProcessorList,\n",
      " |          ...     MinLengthLogitsProcessor,\n",
      " |          ...     TopKLogitsWarper,\n",
      " |          ...     TemperatureLogitsWarper,\n",
      " |          ...     BeamSearchScorer,\n",
      " |          ... )\n",
      " |          >>> import torch\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |          >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |          >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |          >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |          >>> # lets run beam search using 3 beams\n",
      " |          >>> num_beams = 3\n",
      " |          >>> # define decoder start token ids\n",
      " |          >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |          >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |          >>> # add encoder_outputs to model keyword arguments\n",
      " |          >>> model_kwargs = {\n",
      " |          ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
      " |          ... }\n",
      " |      \n",
      " |          >>> # instantiate beam scorer\n",
      " |          >>> beam_scorer = BeamSearchScorer(\n",
      " |          ...     batch_size=1,\n",
      " |          ...     max_length=model.config.max_length,\n",
      " |          ...     num_beams=num_beams,\n",
      " |          ...     device=model.device,\n",
      " |          ... )\n",
      " |      \n",
      " |          >>> # instantiate logits processors\n",
      " |          >>> logits_processor = LogitsProcessorList([\n",
      " |          ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)\n",
      " |          ... ])\n",
      " |          >>> # instantiate logits processors\n",
      " |          >>> logits_warper = LogitsProcessorList([\n",
      " |          ...     TopKLogitsWarper(50),\n",
      " |          ...     TemperatureLogitsWarper(0.7),\n",
      " |          ... ])\n",
      " |      \n",
      " |          >>> outputs = model.beam_sample(\n",
      " |          ...     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs\n",
      " |          ... )\n",
      " |      \n",
      " |          >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      " |  \n",
      " |  beam_search(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation_beam_search.BeamScorer, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = None, **model_kwargs) -> Union[transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences for models with a language modeling head using beam search decoding.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          beam_scorer (:obj:`BeamScorer`):\n",
      " |              An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
      " |              constructed, stored and sorted during generation. For more information, the documentation of\n",
      " |              :class:`~transformers.BeamScorer` should be read.\n",
      " |          logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
      " |              An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
      " |              :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
      " |              head applied at each generation step.\n",
      " |          stopping_criteria (:obj:`StoppingCriteriaList`, `optional`):\n",
      " |              An instance of :class:`~transformers.StoppingCriteriaList`. List of instances of class derived from\n",
      " |              :class:`~transformers.StoppingCriteria` used to tell if the generation loop should stop.\n",
      " |          max_length (:obj:`int`, `optional`, defaults to 20):\n",
      " |              **DEPRECATED**. Use :obj:`logits_processor` or :obj:`stopping_criteria` directly to cap the number of\n",
      " |              generated tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `padding` token.\n",
      " |          eos_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `end-of-sequence` token.\n",
      " |          output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
      " |          return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      " |          synced_gpus (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
      " |              model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.generation_utilsBeamSearchDecoderOnlyOutput`,\n",
      " |          :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
      " |          :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
      " |          ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
      " |          :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if\n",
      " |          ``model.config.is_encoder_decoder=True``.\n",
      " |      \n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> from transformers import (\n",
      " |          ...    AutoTokenizer,\n",
      " |          ...    AutoModelForSeq2SeqLM,\n",
      " |          ...    LogitsProcessorList,\n",
      " |          ...    MinLengthLogitsProcessor,\n",
      " |          ...    BeamSearchScorer,\n",
      " |          ... )\n",
      " |          >>> import torch\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |          >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |          >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |          >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      \n",
      " |          >>> # lets run beam search using 3 beams\n",
      " |          >>> num_beams = 3\n",
      " |          >>> # define decoder start token ids\n",
      " |          >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |          >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |          >>> # add encoder_outputs to model keyword arguments\n",
      " |          >>> model_kwargs = {\n",
      " |          ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
      " |          ... }\n",
      " |      \n",
      " |          >>> # instantiate beam scorer\n",
      " |          >>> beam_scorer = BeamSearchScorer(\n",
      " |          ...     batch_size=1,\n",
      " |          ...     num_beams=num_beams,\n",
      " |          ...     device=model.device,\n",
      " |          ... )\n",
      " |      \n",
      " |          >>> # instantiate logits processors\n",
      " |          >>> logits_processor = LogitsProcessorList([\n",
      " |          ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
      " |          ... ])\n",
      " |      \n",
      " |          >>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
      " |      \n",
      " |          >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      " |  \n",
      " |  generate(self, input_ids: Optional[torch.LongTensor] = None, max_length: Optional[int] = None, min_length: Optional[int] = None, do_sample: Optional[bool] = None, early_stopping: Optional[bool] = None, num_beams: Optional[int] = None, temperature: Optional[float] = None, top_k: Optional[int] = None, top_p: Optional[float] = None, repetition_penalty: Optional[float] = None, bad_words_ids: Optional[Iterable[int]] = None, bos_token_id: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, length_penalty: Optional[float] = None, no_repeat_ngram_size: Optional[int] = None, encoder_no_repeat_ngram_size: Optional[int] = None, num_return_sequences: Optional[int] = None, max_time: Optional[float] = None, max_new_tokens: Optional[int] = None, decoder_start_token_id: Optional[int] = None, use_cache: Optional[bool] = None, num_beam_groups: Optional[int] = None, diversity_penalty: Optional[float] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, forced_bos_token_id: Optional[int] = None, forced_eos_token_id: Optional[int] = None, remove_invalid_values: Optional[bool] = None, synced_gpus: Optional[bool] = None, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
      " |      multinomial sampling, beam-search decoding, and beam-search multinomial sampling.\n",
      " |      \n",
      " |      Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the\n",
      " |      attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values\n",
      " |      indicated are the default values of those config.\n",
      " |      \n",
      " |      Most of these parameters are explained in more detail in `this blog post\n",
      " |      <https://huggingface.co/blog/how-to-generate>`__.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      " |              The sequence used as a prompt for the generation. If :obj:`None` the method initializes it with\n",
      " |              :obj:`bos_token_id` and a batch size of 1.\n",
      " |          max_length (:obj:`int`, `optional`, defaults to :obj:`model.config.max_length`):\n",
      " |              The maximum length of the sequence to be generated.\n",
      " |          max_new_tokens (:obj:`int`, `optional`, defaults to None):\n",
      " |              The maximum numbers of tokens to generate, ignore the current number of tokens. Use either\n",
      " |              :obj:`max_new_tokens` or :obj:`max_length` but not both, they serve the same purpose.\n",
      " |          min_length (:obj:`int`, `optional`, defaults to 10):\n",
      " |              The minimum length of the sequence to be generated.\n",
      " |          do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether or not to use sampling ; use greedy decoding otherwise.\n",
      " |          early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
      " |          num_beams (:obj:`int`, `optional`, defaults to 1):\n",
      " |              Number of beams for beam search. 1 means no beam search.\n",
      " |          temperature (:obj:`float`, `optional`, defaults to 1.0):\n",
      " |              The value used to module the next token probabilities.\n",
      " |          top_k (:obj:`int`, `optional`, defaults to 50):\n",
      " |              The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      " |          top_p (:obj:`float`, `optional`, defaults to 1.0):\n",
      " |              If set to float < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or\n",
      " |              higher are kept for generation.\n",
      " |          repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
      " |              The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
      " |              <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
      " |          pad_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `padding` token.\n",
      " |          bos_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `beginning-of-sequence` token.\n",
      " |          eos_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `end-of-sequence` token.\n",
      " |          length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
      " |              Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
      " |              model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
      " |              sequences.\n",
      " |          no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to int > 0, all ngrams of that size can only occur once.\n",
      " |          encoder_no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
      " |              If set to int > 0, all ngrams of that size that occur in the ``encoder_input_ids`` cannot occur in the\n",
      " |              ``decoder_input_ids``.\n",
      " |          bad_words_ids(:obj:`List[List[int]]`, `optional`):\n",
      " |              List of token ids that are not allowed to be generated. In order to get the tokens of the words that\n",
      " |              should not appear in the generated text, use :obj:`tokenizer(bad_word,\n",
      " |              add_prefix_space=True).input_ids`.\n",
      " |          num_return_sequences(:obj:`int`, `optional`, defaults to 1):\n",
      " |              The number of independently computed returned sequences for each element in the batch.\n",
      " |          max_time(:obj:`float`, `optional`, defaults to None):\n",
      " |              The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
      " |              finish the current pass after allocated time has been passed.\n",
      " |          attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      " |              Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for\n",
      " |              tokens that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same\n",
      " |              shape as :obj:`input_ids` that masks the pad token. `What are attention masks?\n",
      " |              <../glossary.html#attention-mask>`__\n",
      " |          decoder_start_token_id (:obj:`int`, `optional`):\n",
      " |              If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
      " |          use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      " |              Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
      " |              speed up decoding.\n",
      " |          num_beam_groups (:obj:`int`, `optional`, defaults to 1):\n",
      " |              Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
      " |              beams. `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
      " |          diversity_penalty (:obj:`float`, `optional`, defaults to 0.0):\n",
      " |              This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
      " |              at a particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is\n",
      " |              enabled.\n",
      " |          prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`, `optional`):\n",
      " |              If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      " |              provided no constraint is applied. This function takes 2 arguments: the batch ID :obj:`batch_id` and\n",
      " |              :obj:`input_ids`. It has to return a list with the allowed tokens for the next generation step\n",
      " |              conditioned on the batch ID :obj:`batch_id` and the previously generated tokens :obj:`inputs_ids`. This\n",
      " |              argument is useful for constrained generation conditioned on the prefix, as described in\n",
      " |              `Autoregressive Entity Retrieval <https://arxiv.org/abs/2010.00904>`__.\n",
      " |          output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
      " |          return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      " |          forced_bos_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the token to force as the first generated token after the :obj:`decoder_start_token_id`.\n",
      " |              Useful for multilingual models like :doc:`mBART <../model_doc/mbart>` where the first generated token\n",
      " |              needs to be the target language token.\n",
      " |          forced_eos_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the token to force as the last generated token when :obj:`max_length` is reached.\n",
      " |          remove_invalid_values (:obj:`bool`, `optional`):\n",
      " |              Whether to remove possible `nan` and `inf` outputs of the model to prevent the generation method to\n",
      " |              crash. Note that using ``remove_invalid_values`` can slow down generation.\n",
      " |          synced_gpus (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |      \n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If the\n",
      " |              model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific\n",
      " |              kwargs should be prefixed with `decoder_`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.file_utils.ModelOutput` or :obj:`torch.LongTensor`: A\n",
      " |          :class:`~transformers.file_utils.ModelOutput` (if ``return_dict_in_generate=True`` or when\n",
      " |          ``config.return_dict_in_generate=True``) or a :obj:`torch.FloatTensor`.\n",
      " |      \n",
      " |              If the model is `not` an encoder-decoder model (``model.config.is_encoder_decoder=False``), the\n",
      " |              possible :class:`~transformers.file_utils.ModelOutput` types are:\n",
      " |      \n",
      " |                  - :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
      " |                  - :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
      " |                  - :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
      " |                  - :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`\n",
      " |      \n",
      " |              If the model is an encoder-decoder model (``model.config.is_encoder_decoder=True``), the possible\n",
      " |              :class:`~transformers.file_utils.ModelOutput` types are:\n",
      " |      \n",
      " |                  - :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput`,\n",
      " |                  - :class:`~transformers.generation_utils.SampleEncoderDecoderOutput`,\n",
      " |                  - :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput`,\n",
      " |                  - :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput`\n",
      " |      \n",
      " |      Examples::\n",
      " |          >>> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
      " |          >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
      " |          >>> # do greedy decoding without providing a prompt\n",
      " |          >>> outputs = model.generate(max_length=40)\n",
      " |          >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |          >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |          >>> document = (\n",
      " |          ... \"at least two people were killed in a suspected bomb attack on a passenger bus \"\n",
      " |          ... \"in the strife-torn southern philippines on monday , the military said.\"\n",
      " |          ... )\n",
      " |          >>> # encode input context\n",
      " |          >>> input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
      " |          >>> # generate 3 independent sequences using beam search decoding (5 beams)\n",
      " |          >>> # with T5 encoder-decoder model conditioned on short news article.\n",
      " |          >>> outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
      " |          >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
      " |          >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
      " |          >>> input_context = \"The dog\"\n",
      " |          >>> # encode input context\n",
      " |          >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      " |          >>> # generate 3 candidates using sampling\n",
      " |          >>> outputs = model.generate(input_ids=input_ids, max_length=20, num_return_sequences=3, do_sample=True)\n",
      " |          >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
      " |          >>> model = AutoModelForCausalLM.from_pretrained(\"ctrl\")\n",
      " |          >>> # \"Legal\" is one of the control codes for ctrl\n",
      " |          >>> input_context = \"Legal My neighbor is\"\n",
      " |          >>> # encode input context\n",
      " |          >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      " |          >>> outputs = model.generate(input_ids=input_ids, max_length=20, repetition_penalty=1.2)\n",
      " |          >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=False)\n",
      " |          >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |          >>> input_context = \"My cute dog\"\n",
      " |          >>> # get tokens of words that should not be generated\n",
      " |          >>> bad_words_ids = tokenizer([\"idiot\", \"stupid\", \"shut up\"], add_prefix_space=True).input_ids\n",
      " |          >>> # encode input context\n",
      " |          >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      " |          >>> # generate sequences without allowing bad_words to be generated\n",
      " |          >>> outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids)\n",
      " |          >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      " |  \n",
      " |  greedy_search(self, input_ids: torch.LongTensor, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = None, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences for models with a language modeling head using greedy decoding.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
      " |              An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
      " |              :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
      " |              head applied at each generation step.\n",
      " |          stopping_criteria (:obj:`StoppingCriteriaList`, `optional`):\n",
      " |              An instance of :class:`~transformers.StoppingCriteriaList`. List of instances of class derived from\n",
      " |              :class:`~transformers.StoppingCriteria` used to tell if the generation loop should stop.\n",
      " |      \n",
      " |          max_length (:obj:`int`, `optional`, defaults to 20):\n",
      " |              **DEPRECATED**. Use :obj:`logits_processor` or :obj:`stopping_criteria` directly to cap the number of\n",
      " |              generated tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `padding` token.\n",
      " |          eos_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `end-of-sequence` token.\n",
      " |          output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
      " |          return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      " |          synced_gpus (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific keyword arguments will be forwarded to the :obj:`forward` function of the\n",
      " |              model. If model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
      " |          :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
      " |          :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput` if\n",
      " |          ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
      " |          :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput` if\n",
      " |          ``model.config.is_encoder_decoder=True``.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> from transformers import (\n",
      " |          ... AutoTokenizer,\n",
      " |          ... AutoModelForCausalLM,\n",
      " |          ... LogitsProcessorList,\n",
      " |          ... MinLengthLogitsProcessor,\n",
      " |          ... )\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |          >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |          >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
      " |          >>> model.config.pad_token_id = model.config.eos_token_id\n",
      " |      \n",
      " |          >>> input_prompt = \"Today is a beautiful day, and\"\n",
      " |          >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |          >>> # instantiate logits processors\n",
      " |          >>> logits_processor = LogitsProcessorList([\n",
      " |          ...     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
      " |          ... ])\n",
      " |      \n",
      " |          >>> outputs = model.greedy_search(input_ids, logits_processor=logits_processor)\n",
      " |      \n",
      " |          >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      " |  \n",
      " |  group_beam_search(self, input_ids: torch.LongTensor, beam_scorer: transformers.generation_beam_search.BeamScorer, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = None, **model_kwargs)\n",
      " |      Generates sequences for models with a language modeling head using beam search decoding.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          beam_scorer (:obj:`BeamScorer`):\n",
      " |              An derived instance of :class:`~transformers.BeamScorer` that defines how beam hypotheses are\n",
      " |              constructed, stored and sorted during generation. For more information, the documentation of\n",
      " |              :class:`~transformers.BeamScorer` should be read.\n",
      " |          logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
      " |              An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
      " |              :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
      " |              head applied at each generation step.\n",
      " |          stopping_criteria (:obj:`StoppingCriteriaList`, `optional`):\n",
      " |              An instance of :class:`~transformers.StoppingCriteriaList`. List of instances of class derived from\n",
      " |              :class:`~transformers.StoppingCriteria` used to tell if the generation loop should stop.\n",
      " |          max_length (:obj:`int`, `optional`, defaults to 20):\n",
      " |              **DEPRECATED**. Use :obj:`logits_processor` or :obj:`stopping_criteria` directly to cap the number of\n",
      " |              generated tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `padding` token.\n",
      " |          eos_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `end-of-sequence` token.\n",
      " |          output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
      " |          return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      " |          synced_gpus (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |      \n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs that will be forwarded to the :obj:`forward` function of the model. If\n",
      " |              model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
      " |          :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
      " |          :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
      " |          :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput` if\n",
      " |          ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
      " |          :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput` if\n",
      " |          ``model.config.is_encoder_decoder=True``.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> from transformers import (\n",
      " |          ...    AutoTokenizer,\n",
      " |          ...    AutoModelForSeq2SeqLM,\n",
      " |          ...    LogitsProcessorList,\n",
      " |          ...    MinLengthLogitsProcessor,\n",
      " |          ...    HammingDiversityLogitsProcessor,\n",
      " |          ...    BeamSearchScorer,\n",
      " |          ... )\n",
      " |          >>> import torch\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      " |          >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      " |      \n",
      " |          >>> encoder_input_str = \"translate English to German: How old are you?\"\n",
      " |          >>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |      \n",
      " |          >>> # lets run diverse beam search using 6 beams\n",
      " |          >>> num_beams = 6\n",
      " |          >>> # define decoder start token ids\n",
      " |          >>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)\n",
      " |          >>> input_ids = input_ids * model.config.decoder_start_token_id\n",
      " |      \n",
      " |          >>> # add encoder_outputs to model keyword arguments\n",
      " |          >>> model_kwargs = {\n",
      " |          ...     \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True)\n",
      " |          ... }\n",
      " |      \n",
      " |          >>> # instantiate beam scorer\n",
      " |          >>> beam_scorer = BeamSearchScorer(\n",
      " |          ...     batch_size=1,\n",
      " |          ...     max_length=model.config.max_length,\n",
      " |          ...     num_beams=num_beams,\n",
      " |          ...     device=model.device,\n",
      " |          ...     num_beam_groups=3\n",
      " |          ... )\n",
      " |      \n",
      " |          >>> # instantiate logits processors\n",
      " |          >>> logits_processor = LogitsProcessorList([\n",
      " |          ...     HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),\n",
      " |          ...     MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),\n",
      " |          ... ])\n",
      " |      \n",
      " |          >>> outputs = model.group_beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)\n",
      " |      \n",
      " |          >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      " |  \n",
      " |  prepare_inputs_for_generation(self, input_ids: torch.LongTensor, **kwargs) -> Dict[str, Any]\n",
      " |      Implement in subclasses of :class:`~transformers.PreTrainedModel` for custom behavior to prepare inputs in the\n",
      " |      generate method.\n",
      " |  \n",
      " |  sample(self, input_ids: torch.LongTensor, logits_processor: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None, logits_warper: Optional[transformers.generation_logits_process.LogitsProcessorList] = None, max_length: Optional[int] = None, pad_token_id: Optional[int] = None, eos_token_id: Optional[int] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, output_scores: Optional[bool] = None, return_dict_in_generate: Optional[bool] = None, synced_gpus: Optional[bool] = None, **model_kwargs) -> Union[transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, torch.LongTensor]\n",
      " |      Generates sequences for models with a language modeling head using multinomial sampling.\n",
      " |      \n",
      " |      Parameters:\n",
      " |      \n",
      " |          input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
      " |              The sequence used as a prompt for the generation.\n",
      " |          logits_processor (:obj:`LogitsProcessorList`, `optional`):\n",
      " |              An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
      " |              :class:`~transformers.LogitsProcessor` used to modify the prediction scores of the language modeling\n",
      " |              head applied at each generation step.\n",
      " |          stopping_criteria (:obj:`StoppingCriteriaList`, `optional`):\n",
      " |              An instance of :class:`~transformers.StoppingCriteriaList`. List of instances of class derived from\n",
      " |              :class:`~transformers.StoppingCriteria` used to tell if the generation loop should stop.\n",
      " |          logits_warper (:obj:`LogitsProcessorList`, `optional`):\n",
      " |              An instance of :class:`~transformers.LogitsProcessorList`. List of instances of class derived from\n",
      " |              :class:`~transformers.LogitsWarper` used to warp the prediction score distribution of the language\n",
      " |              modeling head applied before multinomial sampling at each generation step.\n",
      " |          max_length (:obj:`int`, `optional`, defaults to 20):\n",
      " |              **DEPRECATED**. Use :obj:`logits_processor` or :obj:`stopping_criteria` directly to cap the number of\n",
      " |              generated tokens. The maximum length of the sequence to be generated.\n",
      " |          pad_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `padding` token.\n",
      " |          eos_token_id (:obj:`int`, `optional`):\n",
      " |              The id of the `end-of-sequence` token.\n",
      " |          output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
      " |              returned tensors for more details.\n",
      " |          output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
      " |              for more details.\n",
      " |          output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
      " |          return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
      " |              Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      " |          synced_gpus (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      " |              Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      " |          model_kwargs:\n",
      " |              Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If\n",
      " |              model is an encoder-decoder model the kwargs should include :obj:`encoder_outputs`.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
      " |          :class:`~transformers.generation_utils.SampleEncoderDecoderOutput` or obj:`torch.LongTensor`: A\n",
      " |          :obj:`torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
      " |          :class:`~transformers.generation_utils.SampleDecoderOnlyOutput` if\n",
      " |          ``model.config.is_encoder_decoder=False`` and ``return_dict_in_generate=True`` or a\n",
      " |          :class:`~transformers.generation_utils.SampleEncoderDecoderOutput` if\n",
      " |          ``model.config.is_encoder_decoder=True``.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> from transformers import (\n",
      " |          ...    AutoTokenizer,\n",
      " |          ...    AutoModelForCausalLM,\n",
      " |          ...    LogitsProcessorList,\n",
      " |          ...    MinLengthLogitsProcessor,\n",
      " |          ...    TopKLogitsWarper,\n",
      " |          ...    TemperatureLogitsWarper,\n",
      " |          ... )\n",
      " |      \n",
      " |          >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      " |          >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |          >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
      " |          >>> model.config.pad_token_id = model.config.eos_token_id\n",
      " |      \n",
      " |          >>> input_prompt = \"Today is a beautiful day, and\"\n",
      " |          >>> input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids\n",
      " |      \n",
      " |          >>> # instantiate logits processors\n",
      " |          >>> logits_processor = LogitsProcessorList([\n",
      " |          ...     MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),\n",
      " |          ... ])\n",
      " |          >>> # instantiate logits processors\n",
      " |          >>> logits_warper = LogitsProcessorList([\n",
      " |          ...     TopKLogitsWarper(50),\n",
      " |          ...     TemperatureLogitsWarper(0.7),\n",
      " |          ... ])\n",
      " |      \n",
      " |          >>> outputs = model.sample(input_ids, logits_processor=logits_processor, logits_warper=logits_warper)\n",
      " |      \n",
      " |          >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_MNLI(syllogism,hypothesis):\n",
    "    \"\"\" Calcul probability than hypothesis is the entailment of syllogism\n",
    "\n",
    "    Args:\n",
    "        syllogism (str): \n",
    "        hypothesis (str): \n",
    "    \n",
    "    Return:\n",
    "        int:probability that hypothesis is the entailment of the premise\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer.encode(syllogism, hypothesis, return_tensors='pt')\n",
    "    logits = model(input_ids)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true \n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    true_prob = probs[:,1].item() * 100\n",
    "    return true_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/py/4rt338cj1ks4z3332m7l3f880000gp/T/ipykernel_54426/2144690394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mentail_contradiction_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentail_contradiction_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrue_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = tokenizer.encode(\"Some models are managers and All models are clerks\", 'All managers are clerks', return_tensors='pt')\n",
    "logits = model(input_ids)[0]\n",
    "\n",
    "# # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "#entailment\" (2) as the probability of the label being true \n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "true_prob = probs[:,1].item() * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6595, 0.5691, 0.5531, 0.4513, 0.5766, 0.5180, 0.4896, 0.3642, 0.5863,\n",
       "         0.3384, 0.7579, 0.6514, 0.6330, 0.3686, 0.2750, 0.5110, 0.3220, 0.5524,\n",
       "         0.4604, 0.3998, 0.5897, 0.6219, 0.5565, 0.4397, 0.3946, 0.2676, 0.5400,\n",
       "         0.5867, 0.6891, 0.5009, 0.4739, 0.6512, 0.4176, 0.4809, 0.2323, 0.5056,\n",
       "         0.6272, 0.3385, 0.4551, 0.5994, 0.3883, 0.4567, 0.3947, 0.3538, 0.4604,\n",
       "         0.4456, 0.3872, 0.4961, 0.3818, 0.3559, 0.5277, 0.5886, 0.6070, 0.4832,\n",
       "         0.3544, 0.5108, 0.6259, 0.3953, 0.4976, 0.3408, 0.3888, 0.4409, 0.6371,\n",
       "         0.5333, 0.4298, 0.5701, 0.4842, 0.2562, 0.6805, 0.4875, 0.5621, 0.5505,\n",
       "         0.5565, 0.3234, 0.5210, 0.4802, 0.5324, 0.4821, 0.6090, 0.3415, 0.5994,\n",
       "         0.2894, 0.5096, 0.4731, 0.3733, 0.4433, 0.5948, 0.6021, 0.6038, 0.4019,\n",
       "         0.3224, 0.3961, 0.5090, 0.4113, 0.3906, 0.6027, 0.4545, 0.4464, 0.5121,\n",
       "         0.4550, 0.5841, 0.4595, 0.4057, 0.5130, 0.4701, 0.3872, 0.4292, 0.3956,\n",
       "         0.4949, 0.4254, 0.4217, 0.5289, 0.4045, 0.3364, 0.7054, 0.3051, 0.3452,\n",
       "         0.2980, 0.3875, 0.7097, 0.4517, 0.5667, 0.5358, 0.5284, 0.6314, 0.5378,\n",
       "         0.6930, 0.4455, 0.5308, 0.4070, 0.3754, 0.4986, 0.4080, 0.5405, 0.7670,\n",
       "         0.3553, 0.2740, 0.5109, 0.6843, 0.5577, 0.7472, 0.4924, 0.6089, 0.4119,\n",
       "         0.3854, 0.6281, 0.5173, 0.5119, 0.3853, 0.3559, 0.4867, 0.5888, 0.5875,\n",
       "         0.6894, 0.5948, 0.7191, 0.5243, 0.4655, 0.3134, 0.5413, 0.5371, 0.4094,\n",
       "         0.4167, 0.5309, 0.6250, 0.5307, 0.4137, 0.5205, 0.6293, 0.4633, 0.5239,\n",
       "         0.4369, 0.4239, 0.3030, 0.4838, 0.4611, 0.4408, 0.7508, 0.7258, 0.6001,\n",
       "         0.2905, 0.3364, 0.3688, 0.4106, 0.6516, 0.4781, 0.3933, 0.5655, 0.4892,\n",
       "         0.6769, 0.4760, 0.5792, 0.4528, 0.4735, 0.5480, 0.6585, 0.4443, 0.6219,\n",
       "         0.4194, 0.3390, 0.4507, 0.4128, 0.5053, 0.5838, 0.4791, 0.4308, 0.4806,\n",
       "         0.4622, 0.4641, 0.6297, 0.4072, 0.3922, 0.6548, 0.4674, 0.4866, 0.5224,\n",
       "         0.4651, 0.4530, 0.4158, 0.5220, 0.5110, 0.4387, 0.4524, 0.3863, 0.2533,\n",
       "         0.6642, 0.5709, 0.5449, 0.3746, 0.4896, 0.5018, 0.5392, 0.2961, 0.5426,\n",
       "         0.6330, 0.5433, 0.6872, 0.4571, 0.4396, 0.5345, 0.2494, 0.3791, 0.5309,\n",
       "         0.5472, 0.3075, 0.5433, 0.4406, 0.4126, 0.5355, 0.5255, 0.2463, 0.6933,\n",
       "         0.6012, 0.6073, 0.5158, 0.6328, 0.4325, 0.6746, 0.5929, 0.4587, 0.4996,\n",
       "         0.4494, 0.3677, 0.6043, 0.4583, 0.6611, 0.7199, 0.5915, 0.4495, 0.4385,\n",
       "         0.4904, 0.5401, 0.5043, 0.4455, 0.4698, 0.4397, 0.5846, 0.5662, 0.5566,\n",
       "         0.6542, 0.4626, 0.6422, 0.4344, 0.4119, 0.4228, 0.5000, 0.5300, 0.6320,\n",
       "         0.5289, 0.4576, 0.5958, 0.7896, 0.5370, 0.3583, 0.6121, 0.6098, 0.4450,\n",
       "         0.3680, 0.4173, 0.6119, 0.4324, 0.5917, 0.4325, 0.4892, 0.6250, 0.7025,\n",
       "         0.4853, 0.4947, 0.4634, 0.4198, 0.5117, 0.4327, 0.4752, 0.4826, 0.6174,\n",
       "         0.4804, 0.4070, 0.3383, 0.5700, 0.6055, 0.2962, 0.5724, 0.3688, 0.5815,\n",
       "         0.5028, 0.5881, 0.5180, 0.4475, 0.4973, 0.4680, 0.4535, 0.4965, 0.4937,\n",
       "         0.4460, 0.5112, 0.6502, 0.4886, 0.4841, 0.3385, 0.5092, 0.5500, 0.5308,\n",
       "         0.6089, 0.4840, 0.5651, 0.6247, 0.4457, 0.6759, 0.6294, 0.6037, 0.4498,\n",
       "         0.6416, 0.5135, 0.4711, 0.6682, 0.6462, 0.5228, 0.4830, 0.5489, 0.4404,\n",
       "         0.5144, 0.5756, 0.5413, 0.3529, 0.4934, 0.5139, 0.7259, 0.5218, 0.5516,\n",
       "         0.5863, 0.5438, 0.3238, 0.4499, 0.6050, 0.3817, 0.3922, 0.6836, 0.3084,\n",
       "         0.5260, 0.3151, 0.7456, 0.4509, 0.5193, 0.4922, 0.6235, 0.4839, 0.4353,\n",
       "         0.4909, 0.5174, 0.5359, 0.5547, 0.5160, 0.5701, 0.4388, 0.5039, 0.6885,\n",
       "         0.7574, 0.5646, 0.3497, 0.6377, 0.5134, 0.4152, 0.6222, 0.6095, 0.5076,\n",
       "         0.5178, 0.3081, 0.5328, 0.4500, 0.3175, 0.5756, 0.5948, 0.5094, 0.3175,\n",
       "         0.6586, 0.4993, 0.5148, 0.5664, 0.4185, 0.5119, 0.5631, 0.7262, 0.5666,\n",
       "         0.5695, 0.3421, 0.3935, 0.3983, 0.6136, 0.3366, 0.5581, 0.5055, 0.4582,\n",
       "         0.4242, 0.4594, 0.6667, 0.5709, 0.6593, 0.4916, 0.4858, 0.4626, 0.5980,\n",
       "         0.6698, 0.5915, 0.5830, 0.4992, 0.3510, 0.3780, 0.3347, 0.5323, 0.4221,\n",
       "         0.4833, 0.3582, 0.3827, 0.4876, 0.4489, 0.4081, 0.4432, 0.7166, 0.4571,\n",
       "         0.5830, 0.3865, 0.6233, 0.5108, 0.5984, 0.6110, 0.4352, 0.5200, 0.6796,\n",
       "         0.4806, 0.5366, 0.7429, 0.3281, 0.5644, 0.5804, 0.4820, 0.4512, 0.4339,\n",
       "         0.4420, 0.5022, 0.5418, 0.5148, 0.4812, 0.3973, 0.4279, 0.5286, 0.3100,\n",
       "         0.4315, 0.6964, 0.4974, 0.4424, 0.6615, 0.2836, 0.3913, 0.5115, 0.5011,\n",
       "         0.4310, 0.5529, 0.3750, 0.5244, 0.3469, 0.6989, 0.4830, 0.6361, 0.4398,\n",
       "         0.3227, 0.5806, 0.4987, 0.5180, 0.4081, 0.6670, 0.4549, 0.5513, 0.5334,\n",
       "         0.3820, 0.4066, 0.4220, 0.4904, 0.3399, 0.4342, 0.4344, 0.5557, 0.4763,\n",
       "         0.6727, 0.6484, 0.5495, 0.5406, 0.6086, 0.6306, 0.4387, 0.4381, 0.4053,\n",
       "         0.4090, 0.4958, 0.6148, 0.3705, 0.5173, 0.4332, 0.4374, 0.6198, 0.5708,\n",
       "         0.5591, 0.4098, 0.5047, 0.4033, 0.5744, 0.5330, 0.5079, 0.4839, 0.4101,\n",
       "         0.4864, 0.4349, 0.6759, 0.5930, 0.3313, 0.7619, 0.4197, 0.6737, 0.5692,\n",
       "         0.3715, 0.4774, 0.5333, 0.5605, 0.6020, 0.5346, 0.7791, 0.4282, 0.5240,\n",
       "         0.3308, 0.4776, 0.7054, 0.6538, 0.5015, 0.4573, 0.5621, 0.5867, 0.4063,\n",
       "         0.4389, 0.4596, 0.4309, 0.6560, 0.4640, 0.4955, 0.3435, 0.5197, 0.4955,\n",
       "         0.7125, 0.2617, 0.6630, 0.4544, 0.5458, 0.4449, 0.4429, 0.6365, 0.5035,\n",
       "         0.6333, 0.4319, 0.4255, 0.4713, 0.5054, 0.5062, 0.4437, 0.5553, 0.5994,\n",
       "         0.6295, 0.3446, 0.4249, 0.6270, 0.5083, 0.6014, 0.4878, 0.4448, 0.4430,\n",
       "         0.5819, 0.4961, 0.4972, 0.4994, 0.3410, 0.4209, 0.4586, 0.5521, 0.5716,\n",
       "         0.4785, 0.2795, 0.5263, 0.5847, 0.4412, 0.7239, 0.5146, 0.3711, 0.6029,\n",
       "         0.4565, 0.5479, 0.5577, 0.5264, 0.4513, 0.5684, 0.4030, 0.3496, 0.4558,\n",
       "         0.5595, 0.3942, 0.4809, 0.6503, 0.4118, 0.6345, 0.4875, 0.5040, 0.3960,\n",
       "         0.4749, 0.3944, 0.5610, 0.5531, 0.5515, 0.4203, 0.4982, 0.3546, 0.3946,\n",
       "         0.5702, 0.5636, 0.4280, 0.5767, 0.3868, 0.2495, 0.4844, 0.4034, 0.5704,\n",
       "         0.4584, 0.5246, 0.4964, 0.6565, 0.3889, 0.4016, 0.5759, 0.4217, 0.3875,\n",
       "         0.7173, 0.4929, 0.5307, 0.3747, 0.3495, 0.4426, 0.3292, 0.4789, 0.4323,\n",
       "         0.5025, 0.5820, 0.4569, 0.5543, 0.3977, 0.5868, 0.5823, 0.5278, 0.5836,\n",
       "         0.6822, 0.4144, 0.7112, 0.7422, 0.4985, 0.4403, 0.5965, 0.6812, 0.4155,\n",
       "         0.5489, 0.5373, 0.5292, 0.5620, 0.3242, 0.6505, 0.4144, 0.5905, 0.4272,\n",
       "         0.5749, 0.6570, 0.5714, 0.4323, 0.2809, 0.6079, 0.5380, 0.4704, 0.2936,\n",
       "         0.4932, 0.4758, 0.4679, 0.4885, 0.5059, 0.6919, 0.4413, 0.4258, 0.4154,\n",
       "         0.6177, 0.6947, 0.6967, 0.5507, 0.6873, 0.5675, 0.4161, 0.4310, 0.4844,\n",
       "         0.3303, 0.4885, 0.4202, 0.6375, 0.4018, 0.5169, 0.7612, 0.4077, 0.5959,\n",
       "         0.4771, 0.5255, 0.3664, 0.4346, 0.5293, 0.3528, 0.5827, 0.5821, 0.5865,\n",
       "         0.4970, 0.5649, 0.3790, 0.4240, 0.4672, 0.3466, 0.3779, 0.5615, 0.3980,\n",
       "         0.3379, 0.4840, 0.3787]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6345e-01, -6.0078e-01,  8.0284e-01,  2.4566e-01,  1.0108e+00,\n",
       "          7.5512e-01, -2.2402e-01, -3.8792e-01, -2.5826e-01, -5.9477e-01,\n",
       "          1.0790e-01, -5.1372e-01, -7.7356e-01,  2.3221e-01, -2.6517e-01,\n",
       "         -5.4540e-01,  4.3621e-01,  7.0442e-01,  3.4869e-01, -8.0230e-01,\n",
       "          2.5517e-01,  1.0233e+00, -5.2839e-01, -4.7353e-01, -1.0822e-01,\n",
       "         -2.0503e-01,  1.1291e+00, -6.2783e-01,  7.5901e-01,  4.5090e-01,\n",
       "          4.5402e-02, -2.6453e-01, -8.6604e-01, -3.8703e-02, -3.1773e-01,\n",
       "          1.0884e-01,  3.7196e-01, -4.8974e-01, -4.0864e-01,  4.3784e-01,\n",
       "         -4.8481e-01, -4.9175e-01,  3.4155e-01, -3.7228e-02, -1.4377e+00,\n",
       "         -1.2239e+00, -3.9218e-01, -6.2047e-01, -2.5454e-01, -6.4233e-01,\n",
       "         -1.1071e+00, -3.3450e-01, -2.1535e-01,  1.5587e-01, -2.1573e-01,\n",
       "          1.0166e+00,  3.7234e-02,  1.0402e-01,  2.2517e-01,  5.0527e-01,\n",
       "         -4.0004e-01, -3.1875e-01,  4.2510e-01,  5.9588e-02,  4.0357e-01,\n",
       "          2.5238e-03,  3.7025e-01, -3.1894e-01,  7.6523e-01, -1.7618e-01,\n",
       "          4.1211e-01,  7.9467e-01, -3.6075e-01, -2.0541e-01,  2.3562e-01,\n",
       "          8.3892e-01, -1.5242e+00,  1.1532e+00,  6.9196e-01, -4.8906e-01,\n",
       "         -2.6736e-01, -1.8681e-01,  5.3275e-01,  1.4531e+00,  2.4415e-01,\n",
       "          4.5044e-01,  3.7749e-01,  1.1879e-01, -1.8162e-01,  1.5929e-01,\n",
       "          1.4554e-01, -1.1659e-03, -6.1048e-01,  6.0694e-01,  2.7800e-01,\n",
       "          2.0442e-01,  2.8761e-01,  2.0023e-01, -1.1418e+00, -2.9634e-01,\n",
       "         -2.0978e-01,  1.0917e-02, -5.9955e-02, -5.1078e-01, -3.6566e-01,\n",
       "          4.8714e-01, -2.6409e-01, -1.1099e+00,  9.3913e-01,  9.2050e-02,\n",
       "         -6.6394e-02,  9.2586e-01,  9.6312e-01, -3.3011e-01, -1.1211e+00,\n",
       "         -1.8494e-01,  4.9074e-01, -3.9138e-01, -2.9970e-01,  4.4517e-01,\n",
       "         -1.0857e-01,  5.7435e-01, -3.5758e-01,  5.3279e-01, -1.1351e+00,\n",
       "          8.0025e-01, -6.2443e-02, -5.8602e-01, -9.2229e-01, -9.6585e-01,\n",
       "          8.3286e-02,  2.0969e-01, -6.4113e-01, -6.6925e-01, -6.5246e-02,\n",
       "         -3.0823e-02, -4.0291e-01,  6.0293e-02,  3.1472e-01,  5.4103e-01,\n",
       "          6.5017e-01, -1.4450e-01,  3.0249e-01, -2.5369e-01,  7.4016e-02,\n",
       "          4.0530e-01,  7.9012e-01, -7.9867e-01, -4.3295e-01, -4.3190e-01,\n",
       "         -6.7402e-01,  6.3761e-01,  3.6021e-03, -8.6758e-01,  5.7246e-01,\n",
       "          4.3429e-01, -2.6504e-02, -6.9899e-01,  1.0386e+00,  4.4334e-01,\n",
       "          4.4326e-01, -6.4860e-02, -4.9175e-02,  5.4708e-01,  9.3964e-01,\n",
       "         -4.3602e-01,  9.7907e-01,  7.3939e-01,  3.8846e-01, -2.4654e-01,\n",
       "         -4.9116e-01, -1.0736e-02,  1.1965e+00,  1.4505e-01,  7.7256e-01,\n",
       "         -3.5555e-01, -3.1843e-01,  2.6271e-01, -1.8948e-01, -4.5364e-01,\n",
       "          4.1592e-01, -1.2916e+00, -4.2602e-02,  2.6063e-01,  2.5381e-01,\n",
       "          6.4997e-01, -6.2479e-01, -2.5704e-01,  3.3150e-01,  2.5138e-01,\n",
       "         -1.2982e+00, -7.3070e-01, -4.0284e-01, -2.8902e-02, -3.1253e-02,\n",
       "         -6.6678e-01,  4.5296e-01, -1.2823e-01,  1.0670e+00, -1.5815e-01,\n",
       "         -4.2700e-01, -1.2493e-01, -1.4437e-01,  3.3565e-01,  5.0596e-01,\n",
       "          3.3593e-01,  2.3925e-01,  2.8912e-01,  1.4014e+00, -6.5626e-01,\n",
       "         -5.2384e-01, -1.0740e+00, -3.6754e-01, -7.5098e-01, -1.1422e-01,\n",
       "         -7.1863e-01, -4.2193e-01,  1.1603e-01,  3.5943e-01,  3.2249e-01,\n",
       "          1.2446e+00,  4.2793e-02,  5.4055e-01, -9.9501e-01,  5.3820e-01,\n",
       "          3.8034e-02, -2.8365e-01, -7.2966e-01,  3.9736e-01,  3.2522e-01,\n",
       "         -3.3760e-01, -1.2794e+00,  3.7257e-01,  4.0519e-01,  3.5275e-02,\n",
       "          1.7319e-01, -7.4855e-01, -1.7832e-02, -7.2987e-02, -3.3581e-01,\n",
       "         -3.6773e-01, -8.6575e-01,  6.5033e-02,  9.4367e-02, -2.3253e-01,\n",
       "          8.0441e-01,  9.0822e-01, -4.5194e-01, -5.5610e-01,  3.2535e-01,\n",
       "         -7.1752e-01,  4.7383e-01, -1.0856e+00, -2.5259e-01, -1.6105e-01,\n",
       "          2.9013e-01, -4.8038e-02,  8.9379e-01,  6.0112e-01, -4.1076e-01,\n",
       "         -2.5598e-01, -5.8518e-01,  4.8632e-01,  8.3048e-01, -4.2106e-01,\n",
       "         -1.1364e-01, -6.9109e-02, -2.3100e-01,  8.7901e-01, -3.8228e-01,\n",
       "          3.1986e-01, -1.8659e-01,  2.4510e-01,  2.9555e-01,  6.8658e-01,\n",
       "         -6.7274e-01,  6.0943e-01, -3.8645e-01, -6.6047e-01,  1.7604e-01,\n",
       "         -3.3029e-01, -5.6444e-02, -1.0988e+00, -2.9406e-01, -5.5408e-01,\n",
       "         -4.6969e-01,  6.1892e-01,  9.9638e-01,  8.0107e-01,  5.4048e-02,\n",
       "         -7.6990e-01,  2.0657e-01,  1.0031e-01, -8.8723e-01,  5.9363e-01,\n",
       "         -2.4084e-01,  9.8650e-02,  3.0268e-01, -1.0246e-02,  9.3061e-02,\n",
       "         -3.8945e-01, -2.0119e-01,  5.0230e-01,  1.1997e-01,  3.9135e-01,\n",
       "         -3.9962e-01,  1.0477e+00, -1.3228e-01, -1.0134e-02, -3.2908e-01,\n",
       "         -1.8995e-01, -3.8892e-01,  1.5918e+00, -8.5079e-01,  2.9850e-01,\n",
       "          5.0626e-01, -2.2197e-02,  3.9515e-01,  1.6547e-02, -5.4038e-01,\n",
       "         -5.6231e-02, -2.3888e-01, -7.0825e-01, -1.8453e-01, -1.7337e-01,\n",
       "          1.0141e+00, -7.6402e-01, -2.9802e-01, -5.7695e-01, -2.6642e-02,\n",
       "          1.7801e-01, -1.4682e-01, -7.1387e-01,  7.3024e-01,  5.1809e-01,\n",
       "         -6.4406e-01,  9.4204e-01, -4.2333e-01, -1.1899e+00, -6.8976e-01,\n",
       "          1.1838e+00,  6.1916e-01, -1.0024e-02,  3.8044e-01, -2.6915e-02,\n",
       "          5.6146e-01,  1.5173e-01,  9.1641e-01,  1.0438e-02,  3.8150e-01,\n",
       "         -4.3330e-02, -2.8766e-01,  5.5238e-01, -4.8922e-01, -6.1781e-02,\n",
       "          9.9663e-01, -3.9222e-01, -1.1404e+00, -1.0055e-01,  8.3854e-01,\n",
       "         -3.3521e-01, -3.0916e-01, -1.1130e-01,  3.6884e-01, -1.5381e-01,\n",
       "          2.0746e-01,  1.2235e-01,  5.3963e-01,  7.5852e-01,  1.6477e-01,\n",
       "         -1.1501e-01, -5.5491e-01, -4.1765e-01, -1.2086e+00, -2.0275e-01,\n",
       "          5.3637e-01,  8.7289e-01, -5.8745e-01, -6.4745e-01,  8.4492e-02,\n",
       "          4.3526e-01, -8.5911e-01, -5.6167e-01,  9.2517e-01,  2.3291e-01,\n",
       "         -1.0109e-01, -1.6289e+00, -4.6076e-02, -7.4618e-01,  5.8619e-02,\n",
       "          9.0795e-01,  1.8294e-01,  9.9216e-01,  9.9023e-02, -9.6534e-02,\n",
       "         -3.2464e-01, -7.3911e-01,  4.9091e-01, -7.1681e-01, -5.5967e-01,\n",
       "         -5.2988e-01, -9.8904e-01, -4.2758e-01,  1.7910e+00, -4.9083e-01,\n",
       "         -5.6003e-01,  2.5866e-01,  4.4908e-01, -1.2055e-01, -4.4311e-01,\n",
       "         -1.2432e-01, -3.4311e-01, -7.4184e-01,  6.5735e-01, -3.4352e-01,\n",
       "          1.7295e-01, -4.7016e-02, -4.0485e-01, -5.2769e-01, -9.2645e-02,\n",
       "          6.1739e-01, -9.7361e-02, -9.3251e-01, -9.2881e-01, -2.1752e+00,\n",
       "          1.6961e-01,  6.1604e-01,  1.2134e-01,  7.3203e-02, -1.8321e-01,\n",
       "         -4.2479e-01, -1.0215e+00, -9.2004e-01, -7.4865e-03, -3.2721e-01,\n",
       "          2.7109e-03,  2.3666e-01, -1.5069e-01,  7.5270e-01, -8.2992e-02,\n",
       "         -1.4557e+00,  1.1163e-01, -4.4856e-02,  2.1905e-01,  5.0726e-01,\n",
       "         -1.1799e-01,  8.1221e-01, -9.8522e-01,  3.0807e-01,  1.5157e-01,\n",
       "          1.7534e-02, -9.2947e-01, -8.8305e-01,  3.9669e-01,  1.0324e-01,\n",
       "         -1.8966e-01, -5.7518e-01,  1.9924e-01,  3.1460e-02, -4.2415e-01,\n",
       "         -5.3033e-02, -7.9800e-01, -7.5363e-01,  6.7714e-01, -6.1797e-01,\n",
       "         -3.3178e-02,  6.6518e-02, -2.6743e-01, -1.1566e-01, -7.1138e-01,\n",
       "         -3.1789e-01,  8.2605e-01,  7.1892e-01,  3.4969e-01, -4.7314e-02,\n",
       "          1.4558e-01, -4.1026e-01,  1.0556e+00,  2.4195e-01, -3.2135e-01,\n",
       "          6.8028e-01,  1.0052e-02, -3.8048e-01, -1.5465e-01,  5.6769e-01,\n",
       "          6.0248e-01, -9.1938e-01, -7.9123e-01, -6.3621e-01, -4.5469e-01,\n",
       "         -7.5665e-01, -7.9789e-02, -7.5321e-01, -1.0344e+00,  5.3853e-01,\n",
       "         -8.3195e-02, -7.1191e-01,  3.6319e-02, -9.5082e-01, -3.0294e-01,\n",
       "          9.2405e-01, -1.5197e+00, -2.9805e-01,  8.6556e-01,  1.9109e-01,\n",
       "          5.1239e-02, -1.3418e-01,  2.7393e-01, -5.6518e-01, -5.8345e-01,\n",
       "         -8.2921e-01,  3.8141e-01, -1.8122e-01, -3.9192e-01, -4.0141e-01,\n",
       "         -1.9588e-01,  1.1178e-01,  7.3629e-01, -4.9786e-01, -2.2608e-01,\n",
       "         -2.1151e-01, -3.0877e-01,  1.0475e+00,  7.2384e-01, -7.8217e-01,\n",
       "         -1.6717e-01, -1.4838e-01, -4.2831e-01, -1.7990e-01,  3.4375e-02,\n",
       "          1.0404e+00,  1.1706e-01, -2.3584e-01, -4.0380e-01, -2.2988e-01,\n",
       "          3.4997e-01,  9.8774e-01,  5.2396e-01, -3.1209e-02, -7.6509e-01,\n",
       "          2.6152e-01,  1.9730e-01, -1.1029e+00, -2.3896e-01,  7.6539e-01,\n",
       "          5.4777e-01,  3.5130e-01,  5.9724e-01, -3.3533e-01, -6.0670e-01,\n",
       "          2.4370e-01,  3.8097e-01, -1.1015e+00,  1.5453e-01, -4.6539e-01,\n",
       "         -2.4831e-01, -7.9544e-03,  4.6174e-01,  1.2023e-01,  6.0717e-01,\n",
       "         -9.9698e-01,  3.9800e-01,  1.7267e-01, -3.5547e-01, -5.1103e-01,\n",
       "         -5.3943e-01, -7.3117e-01, -5.6791e-02, -8.1740e-02, -4.1040e-01,\n",
       "          4.3523e-02,  6.4333e-01,  3.9635e-01, -9.6652e-01,  3.4173e-01,\n",
       "         -1.4542e+00,  8.3910e-01,  6.9635e-02,  2.2886e-01,  4.9253e-01,\n",
       "          6.1127e-01, -1.8567e-03,  1.6141e-02, -2.2272e-01, -2.1050e-02,\n",
       "          5.5501e-01, -5.0180e-01, -1.0227e+00,  1.8994e-01,  1.6939e+00,\n",
       "          3.6431e-01,  6.6391e-01, -6.6454e-01, -4.7288e-01,  7.6080e-01,\n",
       "         -8.7643e-01, -3.1122e-01,  8.7129e-01, -6.2730e-01, -5.6213e-01,\n",
       "         -1.5262e+00,  7.9383e-01,  5.8741e-01,  9.5878e-01,  5.0774e-01,\n",
       "          3.1631e-01, -8.6930e-01, -8.6535e-02,  3.6569e-02,  1.2020e+00,\n",
       "          3.0332e-02, -1.3304e-01, -1.3404e+00,  2.4761e-01, -3.2327e-01,\n",
       "         -8.0248e-02, -6.0201e-01,  5.1900e-01,  7.3157e-01,  3.4334e-01,\n",
       "         -5.0100e-02, -3.1778e-02, -4.3965e-01,  7.8473e-01,  1.3360e-01,\n",
       "          2.9603e-01,  4.4740e-02,  4.4958e-01, -4.5050e-01, -4.4579e-01,\n",
       "          1.6940e-01, -5.8954e-02, -1.3011e-01,  3.4048e-01,  4.9652e-01,\n",
       "          3.1547e-01,  7.6315e-01,  3.9523e-01, -4.6275e-01,  7.7630e-01,\n",
       "          1.9873e-02,  8.8168e-01,  1.6238e-01, -1.1183e-01, -2.9666e-01,\n",
       "         -5.8346e-01, -5.5957e-01,  8.1987e-01,  4.6030e-01,  7.1226e-01,\n",
       "         -2.0949e-01,  2.4405e-01,  1.1799e-01,  4.7317e-01, -7.4813e-02,\n",
       "          8.0429e-01,  8.1447e-01,  1.7448e-01,  4.1601e-01, -8.3780e-01,\n",
       "         -5.1178e-02, -1.3161e-01, -1.1207e-01,  1.4318e-01, -1.2506e+00,\n",
       "         -9.5043e-01,  9.4949e-01, -3.0244e-02,  2.7455e-01,  5.5147e-01,\n",
       "         -5.3998e-01, -3.6914e-01,  2.9969e-02, -5.6917e-01, -1.7410e-01,\n",
       "          8.4992e-01,  2.2906e-01, -2.4610e-02,  4.1897e-01,  6.9975e-01,\n",
       "          1.1273e-01, -1.4134e-01, -1.6648e-01, -8.2902e-02, -9.2086e-02,\n",
       "         -1.3298e+00, -2.9136e-01,  1.4176e-01, -3.2250e-01, -3.1253e-01,\n",
       "          1.1439e-01,  8.2642e-01, -8.3147e-01, -3.2367e-01,  2.6820e-01,\n",
       "         -1.8115e-01, -2.4781e-01, -3.3315e-01,  7.2855e-01, -1.8581e+00,\n",
       "         -4.9751e-01,  1.0493e-01, -1.2043e+00,  3.5092e-01, -2.0161e-01,\n",
       "         -1.4415e+00,  5.8705e-01,  1.1559e+00,  1.0275e+00, -8.4606e-01,\n",
       "          7.7197e-01,  5.8289e-01,  5.6465e-01, -7.3942e-01,  1.4137e-01,\n",
       "          2.1431e+00, -4.3418e-01, -6.8360e-01,  4.7967e-01,  2.2395e-02,\n",
       "          8.5903e-04, -5.6232e-02,  5.5391e-02, -6.4853e-01, -1.3461e-01,\n",
       "          1.3319e+00,  4.8751e-01, -2.3912e-01,  2.4590e-01, -4.6797e-01,\n",
       "         -4.1070e-01,  4.1120e-01, -4.7356e-01,  5.9039e-01,  2.8690e-01,\n",
       "         -1.9947e-01, -4.7432e-01,  5.5035e-02, -1.1564e-01, -1.0055e+00,\n",
       "          3.2892e-01, -4.5711e-01, -3.6052e-01,  2.1591e-01, -2.4030e-01,\n",
       "         -7.5290e-01, -1.2200e+00, -1.5445e-01,  3.8908e-01,  9.7607e-01,\n",
       "          1.0415e+00,  1.0293e+00, -1.9628e-01,  2.5019e-01,  1.0901e+00,\n",
       "         -1.4505e-01, -7.6333e-01,  7.5135e-01, -1.2232e-01,  8.1947e-01,\n",
       "          1.1630e+00, -1.7404e-01,  4.1523e-01,  1.5154e+00,  2.0100e-01,\n",
       "         -5.9525e-01, -1.2207e-01,  1.6283e-01]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_rep "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dddc4829cdb8a95c4dcd99a91d3cecc91af3578de6eb6a763a56b0c106b3b01d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
